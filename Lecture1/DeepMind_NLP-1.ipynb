{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/oxford-cs-deepnlp-2017/lectures\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 목차\n",
    "* Introduction\n",
    "* Lexical Semantics 어휘 의미론\n",
    "* RNNs and Language Modelling\n",
    "* Text Classification\n",
    "* RNNs and GPUs\n",
    "* Sequence Transduction\n",
    "* Speech\n",
    "* Question Answering\n",
    "* Memory\n",
    "* Linguistic Structure\n",
    "* Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lecture 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Neural Networks[Wang Ling]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "숫자, 변수, 함수의 개념 설명, 문제는 우리가 함수(=세상의 이치)를 모름, 따라서 데이터를 통해 유출, 파라미터 개념 설명\n",
    "\n",
    "Model: $y = wx + b$\n",
    "\n",
    "* Input($x$) - Fixed, comes from data\n",
    "* Parameters($w, b$) - Need to bo estimated\n",
    "\n",
    "어떻게 Parameters를 찾을까?\n",
    "\n",
    "여러가지의 모델을 만들음, input 넣고, $\\hat{y}$ 구함, 반복하게 됨, 그런데 이 모델이 맞는지 어떻게 평가하는가?\n",
    "\n",
    "그래서 Cost Function의 개념 등장: How good or bad are the models? 모델의 지표 같은거임\n",
    "\n",
    "Square Loss: $C(w, b) = \\sum(y_n-\\hat{y}_n)^2$ 낮을 수록 좋은 모델이다 \n",
    "\n",
    "이제 Parameters를 찾는 작업을 시작한다. \n",
    "\n",
    "Optimizer 개념 등장: 제일 적절한 $w$, $b$를 찾음 $\\arg \\min C(w, b)$\n",
    "\n",
    "어떻게 찾을까? $w, b$를 조금씩 변화해서 Cost 계산후에 제일 작은 값 찾기!\n",
    "\n",
    "Optimizer Step Size: Big vs Small\n",
    "* Big: Worse minimum, but gets there faster\n",
    "* Small: Better minimun, but gets there slowly\n",
    "\n",
    "조금씩 테스트 하기에는 너무 오래걸림\n",
    "\n",
    "Gredient: 변수가 조금 변화했을 때, 함수 값의 변화량"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "직선만으로 데이터를 설명할 수 없을 때...\n",
    "\n",
    "Underfitting: 모델이 데이터를 설명할 수 없는 상황?!\n",
    "\n",
    "### Nonelinear Neural Models\n",
    "$y=s_1(w_1x+b_1) + s_2(w_2x+b_2)$\n",
    "\n",
    "$s_1 = \\begin{cases} 1& \\sigma(x)<6 \\\\ 0& otherwise\\end{cases}$\n",
    "$s_2 = \\begin{cases} 1& \\sigma(x)\\geq6 \\\\ 0& otherwise\\end{cases}$\n",
    "\n",
    "Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Multilayer perceptron\n",
    "* Underfitting: task is too complex to the model\n",
    "* Overfitting: model is complex, but task is simple\n",
    "\n",
    "데이터가 많을 수록 Overfitting될 걱정 안해도 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "word -> numbers 에 대한 방법들\n",
    "\n",
    "* Word Embedding\n",
    "\n",
    "Continuous - values Sparse(embeddings)\n",
    "\n",
    "=> \n",
    "\n",
    "MLP\n",
    "\n",
    "=>\n",
    "\n",
    "Continuous linear Sparse(softmax) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

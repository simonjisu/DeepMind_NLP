{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Vectors and Lexical Semantics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How to represent Words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Natural language text = sequences of discrete symbols 이산 기호들의 배열(시퀀스)\n",
    "\n",
    "* Navie representaion: one hot vectors $\\in$ $R^{vocabulary}$, one hot 인코딩된 벡터들로 표현 아주큼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>딥마인드</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>워드</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>벡터</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      0  1  2\n",
       "딥마인드  1  0  0\n",
       "워드    0  1  0\n",
       "벡터    0  0  1"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "words = ['딥마인드', '워드', '벡터']\n",
    "df = pd.DataFrame(np.eye(len(words)), index=words, dtype=np.int)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Classical IR: document and query vectors are superpositions of word vectors\n",
    "$$\\hat{d_q}=\\underset{d}{\\arg \\max} \\sim(d,q)$$\n",
    "\n",
    "* Similarly for word classification problems(e.g. Navie Bayes topic models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Issues: sparse, orthogonal representations, semantically weak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Semantic similarity 의미론적 유사성\n",
    "* 더 풍부하게 단어를 표현하고 싶다!!\n",
    "* Distributional semantics: 분산 의미론\n",
    "    * Idea: produce dense vector representations based on the contex/use of words\n",
    "    * Approaches:\n",
    "        * count-based\n",
    "        * predictive\n",
    "        * task-based"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Count-based methods\n",
    "Define a basis vocabulary C of context words \n",
    "* 고를 때는 linguistic intutition(언어적 직관, 주관적인) / statistics of the corpus 에 의해 고름\n",
    "* 이것을 하는 이유는 a, the 같은 의미와 무관한 function word를 포함시키지 않기 위함\n",
    "\n",
    "Define a word window size $w$.\n",
    "\n",
    "Count the basis vocabulary words occurring $w$ words to the left or right of each instance of a target word in the corpus\n",
    "\n",
    "From a vector representation of the target word based on these counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from collections import Counter, defaultdict\n",
    "from operator import itemgetter\n",
    "def get_vocabulary_dict(contexts, stopwords):\n",
    "    vocabulary = Counter()\n",
    "    for sentence in contexts:\n",
    "        words = [word for word in sentence.split() if word not in stopwords] \n",
    "        vocabulary.update(words)\n",
    "    return vocabulary\n",
    "\n",
    "def represent_vector(contexts_words, vocabulary):\n",
    "    vocab_len = len(vocabulary)\n",
    "    word2idx = {w: i for i, w in enumerate(vocabulary)}\n",
    "    count_based_vector = defaultdict()\n",
    "\n",
    "    for key_word, context_w in contexts_words.items():\n",
    "        temp = np.zeros(vocab_len, dtype=np.int)\n",
    "        for w in context_w:\n",
    "            temp[word2idx[w]] += 1\n",
    "        count_based_vector[key_word] = temp\n",
    "    return count_based_vector, word2idx\n",
    "\n",
    "contexts = ['and the cute kitten purred and then',\n",
    "            'the cute furry cat purred and miaowed',\n",
    "            'that the small kitten miaowed and she',\n",
    "            'the loud furry dog ran and bit']\n",
    "stopwords=['and', 'then', 'she', 'that', 'the', 'cat', 'dog', 'kitten']\n",
    "contexts_words = {'kitten': {'cute', 'purred', 'small', 'miaowed'},\n",
    "                  'cat': {'cute', 'furry', 'miaowed'},\n",
    "                  'dog': {'loud', 'furry', 'ran', 'bit'}}\n",
    "\n",
    "vocabulary = get_vocabulary_dict(contexts, stopwords)\n",
    "count_based_vector, word2idx = represent_vector(contexts_words, vocabulary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cute</th>\n",
       "      <th>purred</th>\n",
       "      <th>furry</th>\n",
       "      <th>miaowed</th>\n",
       "      <th>small</th>\n",
       "      <th>loud</th>\n",
       "      <th>ran</th>\n",
       "      <th>bit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitten</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cute  purred  furry  miaowed  small  loud  ran  bit\n",
       "cat        1       0      1        1      0     0    0    0\n",
       "dog        0       0      1        0      0     1    1    1\n",
       "kitten     1       1      0        1      1     0    0    0"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_idx_list = [w for i, w in sorted([(i, w) for w, i in word2idx.items()], key=itemgetter(0))]\n",
    "df = pd.DataFrame(count_based_vector, index=word_idx_list)\n",
    "df.T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare as similarity kernel:\n",
    "$cosine(u, v) = \\dfrac{u\\cdot v}{\\|u\\|\\times\\|v\\|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cosine(u, v):\n",
    "    return np.dot(u, v) / (np.linalg.norm(u) * np.linalg.norm(v))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kitten-cat: 0.57735026919\n",
      "kitten-dog: 0.0\n",
      "cat-dog: 0.288675134595\n"
     ]
    }
   ],
   "source": [
    "print('kitten-cat:', cosine(df['kitten'], df['cat']))\n",
    "print('kitten-dog:',cosine(df['kitten'], df['dog']))\n",
    "print('cat-dog:',cosine(df['cat'], df['dog']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count-based method는 Navie Approach으로 접근\n",
    "\n",
    "Not all features are equal: we must distinguish counts that are high, because they are informative from those that are just independently frequent contexts.\n",
    "\n",
    "Many Normalisation methods: TF-IDF, PMI, etc\n",
    "\n",
    "Some remove the need for norm-invariant similarity metrics\n",
    "\n",
    "But... perhaps there are easier ways to address this problem of count-based mothods(and others, e.g. choice of basis context)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Embedding Models\n",
    "Learning count based vecotrs produces an embedding matrix in $R^{|vocab|\\times|context|}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cute</th>\n",
       "      <th>purred</th>\n",
       "      <th>furry</th>\n",
       "      <th>miaowed</th>\n",
       "      <th>small</th>\n",
       "      <th>loud</th>\n",
       "      <th>ran</th>\n",
       "      <th>bit</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>cat</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dog</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>kitten</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        cute  purred  furry  miaowed  small  loud  ran  bit\n",
       "cat        1       0      1        1      0     0    0    0\n",
       "dog        0       0      1        0      0     1    1    1\n",
       "kitten     1       1      0        1      1     0    0    0"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.T  # df = E"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Rows are word vectors, so we can retrieve them with one hot vectors in $\\{0,1\\}^{|vocab|}$\n",
    "\n",
    "$$onehot_{cat} = \\begin{bmatrix} 0 \\newline 1 \\newline 0 \\end{bmatrix}, cat=onehot_{cat}^TE$$\n",
    "\n",
    "Symbols = unique vectors. Representation = embedding symbols with $E$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(One) generic(포괄적인) idea behind embedding learning:\n",
    "1. Collect instances $t_i \\in inst(t)$ of a word $t$ of vocab $V$\n",
    "2. For each instance, collect its context words $c(t_i)$ (e.g. k-word window)\n",
    "3. Define some score function $score(t_i, c(t_i); \\theta, E)$ with upper bound on output\n",
    "4. Define a loss: \n",
    "$$L=-\\sum_{t\\in V}\\sum_{t_i \\in inst(t)}score(t_i, c(t_i);\\theta,E)$$\n",
    "5. Estimate:\n",
    "$$\\hat{\\theta}, \\hat{E}=\\underset{\\theta, E}{\\arg \\min}\\ L$$\n",
    "6. Use the estimated $E$ as your embedding matrix\n",
    "\n",
    "\n",
    "* the generic algorithm will explore various instances of in the form of the collobert and weston 2011 embeddings word Tyvek models\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Problems: Scoring function\n",
    "\n",
    "Easy to design a useless scorer(e.g. ignore input, output upper bound)\n",
    "\n",
    "Implicitly define is useful\n",
    "\n",
    "Ideally, scorer:\n",
    "* Embeds $t_i$ with $E$\n",
    "* Produces a score which is a function of how well $t_i$ is accounted for by $c(t_i)$, and/or vice versa\n",
    "* Requires the word to account for the context(or the reverse) more than another word in the same place.\n",
    "* Produces a loss which is differentiable w.r.t. $\\theta$ and $E$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### C&W(Collobert et al. 2011)\n",
    "[paper](http://www.jmlr.org/papers/volume12/collobert11a/collobert11a.pdf)\n",
    "\n",
    "Interpretation: representations carry information about what neighbouring representations should look like\n",
    "\n",
    "where it belongs? 같은 정보를 포함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CBoW (Mikolov et al. 2013)\n",
    "[paper](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "Embed context words. Add them.\n",
    "\n",
    "Project back to vocabulary size. Softmax.\n",
    "$$softmax(l)_i=\\dfrac{e^{l_i}}{\\sum_{j}e^{l_i}}$$\n",
    "$$\\begin{eqnarray} P(t_i|context(t_i) & = & softmax(\\sum_{t_j\\in context(t_i)} onehot_{t_j}^{t}\\cdot E\\cdot W_v) \\newline\n",
    "& = & softmax((\\sum_{t_j\\in context(t_i)} onehot_{t_j}^{t}\\cdot E)\\cdot W_v) \\end{eqnarray}$$\n",
    "\n",
    "Minimize Negative Log Likelihood:\n",
    "$$L_{data} = -\\sum_{t_i \\in data}\\log P(t_i|context(t_i))$$\n",
    "\n",
    "장점: \n",
    "* All linear, so very fast. Basically a cheap way of applying one matrix to all inputs.\n",
    "* Historically, negative sampling used instead of expensive softmax.\n",
    "* NLL(negative log-likelihood) minimisation is more stable and is fast enough today\n",
    "* Variants: postion specific matrix per input(Ling et al. 2015)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Skip-gram (Mikolov et al. 2013)\n",
    "[paper](https://arxiv.org/abs/1301.3781)\n",
    "\n",
    "Target word predicts context words.\n",
    "\n",
    "Embed target word.\n",
    "\n",
    "Project into vocabulary. Softmax.\n",
    "$$P(t_j|t_i) = softmax(onehot_{t_i}^T\\cdot E \\cdot W_v$$\n",
    "\n",
    "Learn to estimate Likelihood of context words.\n",
    "$$-\\log P(context(t_i)|t_i) = -\\log \\prod_{t_j\\in context(t_i)}P(t_j|t_i) - \\sum_{t_j\\in context(t_i)}\\log P(t_j|t_i)$$\n",
    "\n",
    "장점:\n",
    "* Fast: One embedding versus |C|(size of contexts) embeddings\n",
    "* Just read off probabilities from softmax\n",
    "* Similiar variants to CBoW possible: position specific projections\n",
    "* Trade off between efficiency and more structured notion of context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "목적이 뭐냐 dense 한 vector 를 얻는 거다\n",
    "\n",
    "Word2Vec은 딥러닝이 아니라 shallow model이다. \n",
    "\n",
    "Word2Vec == PMI Matrix factorization of count based models(Levy and Goldberg, 2014)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific Benefits of Neural Approaches\n",
    "* Easy to learn, especially with good linear algebra libraries.\n",
    "* Highly parallel problem: minibatching, GPUs, distributed models.\n",
    "* Can predict other discrete aspects of context(dependencies, POS tags, etc). Can estimate these probabilities with counts, but sparsity quickly becomes a problems.\n",
    "* Can predict/condition on continuous contexts: e.g. images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating Word Representations\n",
    "Intrinsic Evaluation:\n",
    "* WordSim-353 (Finkelstein et al. 2003)\n",
    "* SimLex-999 (Hill et al 2016, but has been around since 2014)\n",
    "* Word analogy task (Mikolov et al. 2013)\n",
    "* Embedding visualisation (nearest neighbours, T-SNE projection)\n",
    "\n",
    "t-SNE visualize, word 2 dimension cluster: http://colah.github.io/posts/2014-07-NLP-RNNs-Representations/\n",
    "\n",
    "Extrinsic Evaluation:\n",
    "* Simply: do your embeddings improve performance on other task(s).\n",
    "* More ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-based Embedding Learning\n",
    "Just saw methods for learning $E$ through minimising a loss.\n",
    "\n",
    "One use for $E$ is to get input features to a neural network from words.\n",
    "\n",
    "Neural network parameters are updated using gradients on loss $L(x, y, \\theta)$:\n",
    "$$\\theta_{t+1} = update(\\theta_t, \\triangledown_{\\theta}L(x, y, \\theta_t)) $$\n",
    "\n",
    "If $E \\subseteq \\theta$ then this update can modify $E$ (if we let it):\n",
    "$$E_{t+1} = update(E_t, \\triangledown_E L(x, y, \\theta_t))$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Task-based Features: Bow Classifiers\n",
    "Classify sentences/documents based on a variable number of word representations\n",
    "\n",
    "Simplest options: bag of vectors\n",
    "$$P(C|D)=softmax(W_C \\sum_{t_i \\in D} embed_E(t_i))$$\n",
    "\n",
    "Projection into logits (input to softmax) canbe arbitrarily complex. E.g.:\n",
    "$$P(C|D)=softmax(W_C \\cdot \\sigma (\\sum_{t_i \\in D} embed_E(t_i)))$$\n",
    "\n",
    "* $C$: class\n",
    "* $D$: document\n",
    "\n",
    "Example tasks:\n",
    "* Sentiment analysis: tweets, movie reviews\n",
    "* Document classification: 20 Newsgroups\n",
    "* Author identification \n",
    "\n",
    "#### Task-based Features: Bilingual Features\n",
    "linguistic general approach: translations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task-based Features: Interpretation\n",
    "Capture information salient to the task, no guarantee this will capture \"general\" meaning beyond features useful for the task\n",
    "\n",
    "This can be overcome by using a multi-task objective but this comes with its own difficulties\n",
    "\n",
    "Alternatively, embeddings can e pretrained and fixed, relying on task-specific projections into the network, but is the pretraining objective general enough???? 의심해봐야함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "데이터가 많으면 그냥 pre-trained할 필요 없이 Embedding을 만든(random initialize) 담에 같이 train하면 됨, 만약에 데이터가 충분치 않다면, 미리 training하는 것이 좋아 보임"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
